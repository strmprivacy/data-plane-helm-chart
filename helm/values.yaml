enableLivenessChecks: false

registry:
  url: "europe-west4-docker.pkg.dev"
  awsMarketplaceUrl: "709825985650.dkr.ecr.us-east-1.amazonaws.com/strm-privacy"
  base:
    prefix: "stream-machine-production"
    path: "docker-public"

  # Fill out the value below or download a prefilled values.yaml from the Console
  # Only required when license type is SELF_HOSTED
  imagePullSecret: ""

license:
  # Through which channel the Helm chart is installed
  installationType: SELF_HOSTED
  # Fill out the values below or download a prefilled values.yaml from the Console
  installationId: "<installation_id>"
  installationClientId: "<client_id>"
  installationClientSecret: "<client_secret>"

serviceAccount: "strmprivacy"

marketplace:
  aws:

api:
  host: "api.strmprivacy.io"
  port: 443
  authHost: "accounts.strmprivacy.io"

prometheus:
  enabled: &prometheusEnabled true
  prometheus:
    prometheusSpec:
      serviceMonitorSelector: {}
      serviceMonitorSelectorNilUsesHelmValues: false
  grafana:
    dashboardProviders:
      dashboardproviders.yaml:
          apiVersion: 1
          providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
    dashboardsConfigMaps:
      default: "grafana-dashboards"

kafka:
  # Note: disable for production usage
  enabled: true
  fullnameOverride: "kafka"
  port: "9092"
  metrics:
    kafka:
      enabled: true
    jmx:
      enabled: true
      existingConfigmap: "kafka-jmx-configuration"
    serviceMonitor:
      enabled: *prometheusEnabled

redis:
  # Note: disable for production usage
  enabled: true
  fullnameOverride: "redis-strm"
  metrics:
    enabled: true
    serviceMonitor:
      enabled: *prometheusEnabled
  global:
    redis:

postgresql:
  # Note: disable for production usage
  enabled: true
  fullnameOverride: "postgres"
  auth:
    # For dev purposes, using the default 'postgres' user is unpractical, since the password changes after uninstalls /
    # fresh installs, mismatching with the Persistent Volume Claims (which are not deleted by default upon uninstall).
    # Therefore we create a different user with static password. (Again, the built-in postgres/redis/kafka are not meant
    # for production usage!)
    username: batch-jobs-user
    password: batch-jobs-pw

services:
  # For routing traffic from outside the cluster
  loadbalancer:
    enabled: false
    port: "443"
    # array of allowed CIDR ranges
    loadBalancerSourceRanges: []
    # arbitrary key value annotations that are added to every Service of type LoadBalancer
    annotations: {}

  # For routing traffic in the cluster
  clusterIp:
    enabled: true

components:
  eventGateway:
    name: event-gateway
    enabled: true
    image:
      name: "events/event-gateway"
      version: "v0.76.0"
    replicas: 1
    resources:
      gateway:
      envoy:
    configuration:
      kafkaAuth:
        user:
        password:
      redisHost:
      redisUser:
      redisPassword:

      # Application port, traffic goes via Envoy to the application
      appPort: "8081"
      envoy:
        port: "8080"
        adminPort: "9000"
        jwtAudience:

  webSocket:
    enabled: true
    image:
      name: "events/web-socket"
      version: "v1.24.0"
    name: web-socket
    replicas: 1
    configuration:
      kafkaAuth:
        user:
        password:
      # Application port, traffic goes via Envoy to the application
      appPort: "8081"
      envoy:
        port: "8080"
        adminPort: "9000"
        jwtAudience:

  batchJobAgent:
    enabled: true
    image:
      name: "management/entity-agents/batch-jobs-agent"
      version: "v1.26.2"
    name: batch-jobs-agent
    configuration:
      postgres:
        host:
        port:
        user: batch-jobs-user
        # Alternatively, provide an existing secret containing the postgres password below
        password:
        # If no secret exists, a password can be specified above, and a secret will be created for it (leave these empty then)
        passwordSecretName: postgres
        passwordSecretKeyName: password
        database:


  batchExportersAgent:
    enabled: true
    name: batch-exporters-agent
    image:
      name: "management/entity-agents/batch-exporters-agent"
      version: "v1.25.2"
    configuration:
      kafkaAuth:
        user:
        password:

  streamsAgent:
    name: streams-agent
    enabled: true
    image:
      name: "management/entity-agents/streams-agent"
      version: "v1.27.2"
    configuration:
      kafkaAuth:
        user:
        password:
      redisSecret:
        name:
        key:

  dataConnectorAgent:
    enabled: true
    name: data-connectors-agent
    image:
      name: "management/entity-agents/data-connectors-agent"
      version: "v1.8.2"
    configuration:

  confluentSchemaProxy:
    enabled: true
    name: confluent-schema-proxy
    image:
      name: "management/entity-agents/confluent-schema-proxy"
      version: "v1.5.1"
    configuration:

  esrProxy:
    enabled: true
    name: esr-proxy
    image:
      name: "management/esr-proxy"
      version: "v1.5.0"
    configuration:
      kafkaAuth:
        user:
        password:

  decrypterConfig:
    name: decrypter-config
    enabled: true
    redisHost:
    configuration:
      kafkaAuth:
        user:
        password:

  batchExporterConfig:
    name: batch-exporter-config
    enabled: true
    configuration:
      kafkaAuth:
        user:
        password:

kafkaSecurityConfig:
  enabled: false
  securityProtocol: "PLAINTEXT"
  # on local file system
  sslTruststoreSecretName: "client-truststore-jks"
